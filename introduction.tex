\section{Introduction}

Let $g \colon [0,1] \times \R \to \R$ be continuous 
and consider the differential equation 
\begin{align}
 \label{eq:ode}
 h(0) & = 0, &
 \D h(t) & = g(t,h(t)) \quad t \in [0,1], 
\end{align}
where $\D h$ denotes the derivative of $h$. 
How complex can the solution~$h$ be, 
assuming that $g$ is polynomial-time computable? 
Here, polynomial-time computability 
and other notions of complexity 
are from the field of 
\emph{Computable Analysis}~\cite{weihrauch00:_comput_analy}
and measure how hard it is to 
approximate real functions with specified precision 
(Section~\ref{section: preliminaries}). 

If we put no assumption on $g$ other than being polynomial-time computable, 
the solution~$h$ (which is not unique in general) can be non-computable. 
Table~\ref{table:related} summarizes known results about 
the complexity of $h$ under various assumptions 
(that get stronger as we go down the table). 
In particular, if $g$ is (globally) Lipschitz continuous, 
then the (unique) solution $h$ is known to be 
polynomial-space computable but still can be 
$\classPSPACE$-hard \cite{kawamura2010lipschitz}. 
In this paper, we study the complexity of $h$ 
when we put stronger assumptions about 
the smoothness of $g$. 

\begin{table}
\renewcommand\arraystretch{1.3}
\begin{center}
 \caption{The complexity of the solution $h$ of \eqref{eq:ode}
 assuming $g$ is polynomial-time computable.}
 \label{table:related}
 \begin{tabular}{lll}
  Assumptions & Upper bounds & Lower bounds \\
  \hline
   --- & --- & can be all non-computable \cite{pour1979computable} \\
  $h$ is the unique solution & computable \cite{coddington1955theory}
  & can take arbitrarily long time \cite{ko1983computational,miller1970recursive} \\
  the Lipschitz condition  & polynomial-space \cite{ko1983computational}
      &	can be $\classPSPACE$-hard \cite{kawamura2010lipschitz}\\
  $g$ is of class $\classC ^{(\infty, 1)}$ & polynomial-space 
      & can be $\classPSPACE$-hard (Theorem~\ref{DifferentiableIsPspace}) \\
  \parbox[t]{11em}{$g$ is of class $\classC ^{(\infty, k)}$\\{}(for each constant $k$)}
  & polynomial-space & can be $\classCH$-hard (Theorem~\ref{KTimesIsCH}) \\
  $g$ is analytic
  & polynomial-time \cite{muller1987uniform,ko1988computing} 
  & ---
 \end{tabular}
\end{center}
\end{table}

In numerical analysis, 
knowledge about smoothness of the input function 
(such as being differentiable enough times) 
is often beneficial 
in applying certain algorithms or simplifying their analysis.
However, 
to our knowledge, 
this casual understanding that smoothness is good 
has not been rigorously substantiated 
in terms of computational complexity theory. 
This motivates us to ask whether, 
for our differential equation \eqref{eq:ode}, 
smoothness really reduces the complexity of the solution. 

At the extreme is the case where $g$ is analytic: 
$h$ is then shown to be polynomial-time computable 
(the last row of the table) 
by an argument based on Taylor series. 
Thus our interest is in 
the cases between Lipschitz and analytic 
(the fourth and fifth rows). 
We say that $g$ is of class $\classC ^{(i, j)}$
if the partial derivative $\D ^{(i, j)} g$ 
(often also denoted $\partial ^{i + j} g (t, y) / \partial t ^i \partial y ^j$)
exists and is continuous%
\footnote{%
Another common terminology is to say that $g$ is of class $\classC ^k$
if it is of class $\classC ^{(i,j)}$ 
for all $i$, $j$ with $i + j \leq k$.}; 
it is said to be of class $\classC ^{(\infty, j)}$ if
it is of class $\classC ^{(i, j)}$ for all $i \in \N$. 

\begin{theorem}
 \label{DifferentiableIsPspace}
There is a polynomial-time computable function
$g \colon [0,1] \times [-1,1] \to \R$ 
of class $\classC ^{(\infty, 1)}$ such that
the equation \eqref{eq:ode} has a 
$\classPSPACE$-hard solution $h \colon [0, 1] \to \R$. 
 \end{theorem}

 \begin{theorem}
  \label{KTimesIsCH}
Let $k$ be a positive integer. 
There is a polynomial-time computable function
$g \colon [0,1] \times [-1,1] \to \R$ 
of class $\classC ^{(\infty, k)}$ such that
the equation \eqref{eq:ode} has a 
$\classCH$-hard solution $h \colon [0, 1] \to \R$, 
where $\classCH \subseteq \classPSPACE$ is the 
Counting Hierarchy (see Section~\ref{subsection: counting hierarchy}). 
 \end{theorem}

We said
$g \colon [0,1] \times [-1, 1] \to \R$ instead of 
$g \colon [0,1] \times \R \to \R$, because
the notion of polynomial-time computability of real functions 
is defined in this paper only when the domain is a bounded closed region. 
This notational choice makes
the equation~\eqref{eq:ode} ill-defined 
in case $h$ ever takes a value outside $[-1, 1]$; 
by saying that $h$ is a solution in Theorem~\ref{DifferentiableIsPspace}, 
we are also claiming that 
$h (t) \in [-1, 1]$ for all $t \in [0, 1]$. 
In any case, 
since we are putting stronger assumptions on $g$ than Lipschitz continuity, 
such a solution $h$, if it exists, is unique. 

The questions of whether smoothness of the input function 
reduces the complexity of the output
have been asked for operations other than solving differential equations, 
and the following negative results are known. 
The integral of a polynomial-time computable real function 
can be $\classNumberP$-hard, and this does not change 
by restricting the input to 
$\classC ^\infty$ (infinitely differentiable) functions
\cite[Theorem~5.33]{ko1991complexity}. 
Similarly, the function obtained by maximization 
from a polynomial-time computable real function 
can be $\classNP$-hard, and this is still so
even if the input function is restricted to $\classC ^\infty$ 
\cite[Theorem~3.7]{ko1991complexity}%
\footnote{%
The proof of this fact in \cite[Theorem 3.7]{ko1991complexity}
needs to be fixed by redefining 
\[f(x) = 
\begin{cases}
 u_s & \text{if not } R(s,t), \\
 u_s + 2^{-(p(n)+2n+1)\cdot n} \cdot h_1(2^{p(n)+2n+1} (x - y_{s,t})) & \text{if } R(s,t). 
\end{cases}\]
}. 
(Restricting to analytic inputs 
renders the output polynomial-time computable, 
again because of the argument based on Taylor series.)
In contrast, 
although we have Theorem~\ref{KTimesIsCH} for each $k$, 
we do not know about the complexity of 
$h$ when $g$ is assumed to be infinitely differentiable. 

\subsubsection*{Notation}
Let $\N$, $\Z$, $\Q$, $\R$ denote the set of natural numbers,
integers,
rational numbers and 
real numbers, respectively.

Let $A$ and $B$ be bounded closed intervals in $\R$.
We write $|f| = \sup_{x \in A} f(x)$ for $f \colon A \to \R$.
A function $f \colon A \to \R$ is \emph{of class $\classC^i$}
($i$-times continuously differentiable)
if all the derivatives $\D f, \D^2 f, \dots, \D^i f$ exist and are continuous.

For a differentiable function $g$ of two variables, 
we write $\D _1 g$ and $\D _2 g$ for the derivatives of $g$ 
with respect to the first and the second variable,
respectively.
A function $g \colon A \times B \to \R$ is of \emph{class $\classC^{(i, j)}$}
if for each $n \in \{0, \dots, i\}$ and $m \in \{0, \dots j\}$,
the derivative $\D_1^n \D_2^m g$ exists and is continuous.
A function $g$ is of \emph{class $\classC^{(\infty, j)}$}
if it is of class $\classC^{(i, j)}$ for all $i \in \N$.
When $g$ is of class $\classC^{(i,j)}$,
we write $\D^{(i,j)}g$ for the derivative $\D_1^i \D_2^j g$.
