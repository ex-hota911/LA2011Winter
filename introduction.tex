\section{Introduction}

Let $g \colon [0,1] \times \R \to \R$ be continuous 
and consider the following differential equation: 
\begin{align}
 \label{eq:ode}
 h(0) & = 0, &
 \D h(t) & = g(t,h(t)) \quad t \in [0,1], 
\end{align}
where $\D h$ denotes the derivative of $h$. 
How complex can the solution~$h$ be, 
assuming that $g$ is polynomial-time computable? 
Here, the polynomial-time computability 
and other notions of complexity 
are from the field of 
\emph{Computable Analysis}~\cite{weihrauch00:_comput_analy}
and measure how hard it is to 
approximate real functions with specified precisions 
(Section~\ref{section: preliminaries}). 

If we put no assumption on $g$ other than being polynomial-time computable, 
the solution~$h$ (which is not unique in general) can be non-computable. 
Table~\ref{table:related} summarizes known results about 
the complexity of $h$ under various assumptions on $g$, 
with the assumptions getting stronger as we go down. 
In particular, if $g$ is (globally) Lipschitz continuous, 
then the (unique) solution $h$ is known to be 
polynomial-space computable but still can be 
$\classPSPACE$-hard \cite{kawamura2010lipschitz}. 
In this paper, we study the complexity of $h$ 
when we put stronger assumptions about 
the smoothness of $g$. 

\begin{table}
\renewcommand\arraystretch{1.3}
\begin{center}
 \caption{The complexity of the solution $h$ of \eqref{eq:ode}
 assuming $g$ is polynomial-time computable.}
 \label{table:related}
 \begin{tabular}{lll}
  Assumptions & Upper bounds & Lower bounds \\
  \hline
   --- & --- & can be all non-computable \cite{pour1979computable} \\
  $h$ is the unique solution & computable \cite{coddington1955theory}
  & can take arbitrarily long time \cite{ko1983computational,miller1970recursive} \\
  the Lipschitz condition  & polynomial-space \cite{ko1983computational}
      &	can be $\classPSPACE$-hard \cite{kawamura2010lipschitz}\\
  $g$ is of class $\classC ^{(\infty, 1)}$ & polynomial-space 
      & \parbox[t]{11em}{can be $\classPSPACE$-hard\\{}(Theorem~\ref{DifferentiableIsPspace})} \\
  \parbox[t]{11em}{$g$ is of class $\classC ^{(\infty, k)}$\\{}(for any constant $k$)}
  & polynomial-space & \parbox[t]{14zw}{can be $\classCH$-hard\\{}(Theorem~\ref{KTimesIsCH})} \\
  $g$ is analytic
  & polynomial-time \cite{muller1987uniform,ko1988computing} 
  & ---
 \end{tabular}
\end{center}
\end{table}

In numerical analysis, 
knowledge about smoothness of the input function 
(such as being differentiable enough times) 
is often beneficial 
in applying certain algorithms or simplifying their analysis.
However, to our knowledge
this casual understanding that smoothness is good 
has not been 
rigorously substantiated in terms of 
computational complexity theory. 
This motivates us to ask whether, 
for our differential equation \eqref{eq:ode}, 
smoothness helps to reduce the complexity of the solution. 

At the extreme is the case where $g$ is analytic: 
as the last row of the table shows, 
$h$ can then be shown to be polynomial-time computable 
by an argument based on Taylor series. 
Thus our interest is in 
the cases between Lipschitz and analytic 
(the fourth and fifth rows in the table). 
We say that $g$ is of class $\classC ^{(i, j)}$
if the partial derivative $\D ^{(i, j)} g$ 
(often also denoted $\partial ^{i + j} g (t, y) / \partial t ^i \partial y ^j$)
exists and is continuous%
\footnote{%
Another common terminology is to say that $g$ is of class $\classC ^k$
if it is of class $\classC ^{(i,j)}$ 
for all $i$, $j$ with $i + j \leq k$.}; 
it is said to be of class $\classC ^{(\infty, j)}$ if
it is of class $\classC ^{(i, j)}$ for all $i \in \N$. 

\begin{theorem}
 \label{DifferentiableIsPspace}
There is a polynomial-time computable function
$g \colon [0,1] \times [-1,1] \to \R$ 
of class $\classC ^{(\infty, 1)}$ such that
the equation \eqref{eq:ode} has a 
$\classPSPACE$-hard solution $h \colon [0, 1] \to \R$. 
 \end{theorem}

 \begin{theorem}
  \label{KTimesIsCH}
Let $k$ be a positive integer. 
There is a polynomial-time computable function
$g \colon [0,1] \times [-1,1] \to \R$ 
of class $\classC ^{(\infty, k)}$ such that
the equation \eqref{eq:ode} has a 
$\classCH$-hard solution $h \colon [0, 1] \to \R$, 
where $\classCH \subseteq \classPSPACE$ is the 
Counting Hierarchy (see Section~\ref{subsection: counting hierarchy}). 
 \end{theorem}

We said
$g \colon [0,1] \times [-1, 1] \to \R$ instead of 
$g \colon [0,1] \times \R \to \R$, because
the notion of polynomial-time computability of real functions 
is defined in this paper only when the domain is bounded closed region. 
This notational choice makes
the validity of the equation~\eqref{eq:ode} ill-defined 
in case $h$ ever takes a value outside $[-1, 1]$; 
by saying that $h$ is a solution in Theorem~\ref{DifferentiableIsPspace}, 
we are implicitly also claiming that 
$h (t) \in [-1, 1]$ for all $t \in [0, 1]$. 
In any case, 
since we are putting stronger assumptions on $g$ than Lipschitz continuity, 
such a solution $h$, if it exists, is unique. 

The questions of whether smoothness of the input function 
reduces the complexity of the output
have been asked for operations other than solving differential equations, 
and some negative results are known. 
The integral of a polynomial-time computable real function 
is $\classNumberP$-hard, and this does not change 
even if the input is restricted to 
$\classC ^\infty$ (infinitely differentiable) functions
\cite[Theorem~5.33]{ko1991complexity}. 
Similarly, the function obtained by maximization 
from a polynomial-time computable real function 
is $\classNP$-hard, and this is still so
even if the input function is restricted to $\classC ^\infty$ 
\cite[Theorem~3.7]{ko1991complexity}%
\footnote{%
The proof of this fact in \cite[Theorem 3.7]{ko1991complexity}
needs to be fixed by redefining 
\[f(x) = 
\begin{cases}
 u_s & \text{if not } R(s,t), \\
 u_s + 2^{-(p(n)+2n+1)\cdot n} \cdot h_1(2^{p(n)+2n+1} (x - y_{s,t})) & \text{if } R(s,t). 
\end{cases}\]
}. 
(Restricting to analytic inputs 
renders the output polynomial-time computable, 
again because of the argument based on Taylor series.)
In contrast, 
although we have shown Theorem~\ref{KTimesIsCH} for each $k$, 
we do not know about the complexity of 
$h$ when $g$ is assumed to be infinitely differentiable. 

\subsubsection*{Notation}
Let $\N$ denote the set of natural numbers,
$\Z$ denote the set of integer numbers,
$\Q$ denote the set of rational numbers 
and $\R$ denote the set of real numbers.

Let $A$ and $B$ be bounded closed intervals in $\R$.
We denote $|f|$ as $\sup_{x \in A} f(x)$ where $f \colon A \to \R$.
A function $f \colon A \to \R$ is \emph{class $\classC^i$}
($i$-times continuously differentiable)
if there exist the derivatives $\D f, \D^2 f, \dots, \D^i f$ and all of them are continuous.
A function $g$ is of \emph{class $\classC^k$} if it is $k$-times continuously differentiable.

Let $g$ be a differentiable function of two variable,
we denote $\D_1 g$ as the derivative of $g$ with respect to the first variable,
and $\D_2 g$ as the derivative of $g$ with respect to the second variable.
A function $g \colon A \times B \to \R$ is of \emph{class $\classC^{(i, j)}$}
($(i, j)$-times continuously  differentiable)
if for each $n \in \{0, \dots, i\}$ and $m \in \{0, \dots j\}$,
there exists the derivative $\D_1^n \D_2^m g$ and it is continuous.
A function $g$ is of \emph{class $\classC^{(\infty, j)}$}
if $g$ is of class $\classC^{(i, j)}$ for all $i \in \N$.
When a function $g$ is of class $\classC^{(i,j)}$,
we write $\D^{(i,j)}g$ for the derivative $\D_1^i \D_2^j g$.
